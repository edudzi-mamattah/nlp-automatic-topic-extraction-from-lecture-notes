# -*- coding: utf-8 -*-
"""nlp_automatic_topic_extraction_of_taught_lectures.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wS0rnOfXjvvJvbUU4JYjZdSrwd4nGSGk
"""

# import google drive because that's where the files are stored
# from google.colab import drive  
# drive.mount('/content/drive')

"""Disclaimer: The data used in this study cannot be published publicly, so to use the code please feel free to use your oen datasets (.pdf files of lecture notes/presentations)

To Check Memory and GPU usage
"""

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

import warnings
warnings.filterwarnings('ignore')

"""Make installations of libraries and packages needed"""

!pip install pdfplumber
!pip install gensim
!pip install summa
!pip install wordcloud
!pip install Pillow
!pip install Counter
!pip install networkx

!pip install --upgrade numpy
!pip install --upgrade scipy
!pip install --upgrade pandas

!pip install matplotlib -qq
!pip install seaborn -qq
!pip install pyLDAvis -qq

"""This chunk will usually fail on the first run due to the WordCloud package not being utilised. 
Please run all again after the runtime stops for the first time.
"""

import os
# os.kill(os.getpid(), 9)


try:
  from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
except (ImportError, KeyError, ModuleNotFoundError):
  ## code to install gem
  print("Stopping RUNTIME. Colaboratory will restart automatically. \nPlease \"Run All\" again to load WordCloud package.")
  os.kill(os.getpid(), 9)
  exit()

"""Import required libraries"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import itertools
from os import path
from PIL import Image
import matplotlib.pyplot as plt
# %matplotlib inline

from nltk import ngrams,bigrams,trigrams
import collections as collect # collect.Counter()
import pdfplumber
import nltk
import networkx as nx
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
import gensim
from gensim.utils import simple_preprocess
from gensim.models import LdaModel, LdaMulticore
from gensim import models
from gensim.models.phrases import Phrases
from gensim.models.phrases import Phraser
import re
import glob
import os
from tqdm.auto import tqdm
import numpy as np
from summa import summarizer
from summa import keywords  
from nltk.stem import SnowballStemmer
from nltk.stem import WordNetLemmatizer

import seaborn as sns
sns.set()
import pyLDAvis.gensim_models
pyLDAvis.enable_notebook()# Visualise inside a notebook
from gensim.models import CoherenceModel
# from wordcloud import WordCloud
from textblob import TextBlob
# import matplotlib.pyplot as plt
# %matplotlib inline

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

"""# Data Entry & Preprocessing

Extract the text from the pdf files and store into a string variable. Then append the strings of the respective pdf files into a list where each list element represents the words extracted from the pdf of a specific module.
"""

# function to extract files

def extract_files(file_directory, file_order, module_list):
  extracted_text = ""
  for current_pdf_file in tqdm(file_directory):
    file_order.append(os.path.relpath(current_pdf_file, "/Data"))
    with pdfplumber.open(current_pdf_file) as my_pdf:
      for page in my_pdf.pages:
        extracted_text  += page.extract_text()
        continue  
      module_list.append(extracted_text)  
      extracted_text = ""  
    
    
  print(file_order)

"""QDA - Quantitative Data Analysis"""

qda_files = glob.glob("/Data/QDA*.pdf")
qda_list = []
qda_file_order = []



extract_files(qda_files, qda_file_order, qda_list)

"""ML - Machine Learning """

ml_list = []
ml_file_order = []


ml_files = glob.glob("/Data/ML*.pdf")


extract_files(ml_files, ml_file_order, ml_list)

"""AI - Artificial Intelligence"""

ai_list = []
ai_file_order = []


ai_files = glob.glob("/Data/AI*.pdf")

extract_files(ai_files, ai_file_order, ai_list)

"""DL - Deep Learning"""

dl_list = []
dl_file_order = []


dl_files = glob.glob("/Data/DL*.pdf")


extract_files(dl_files, dl_file_order, dl_list)

"""


MD - Modern Data
"""

md_list = []
md_file_order = []


md_files = glob.glob("/Data/MD*.pdf")


extract_files(md_files, md_file_order, md_list)

"""RPM - Research Proj. Mgmt"""

rpm_list = []
rpm_file_order = []


rpm_files = glob.glob("/Data/RPM*.pdf")

extract_files(rpm_files, rpm_file_order, rpm_list)

"""# Data Cleaning

Remove the characters that are email addresses, web urls, pollev links and characters that are not alphabets, a newline character, or a space character. This is the first step of cleaning
"""

def preprocess (module_list):
  module_list = [re.sub(re.compile('\S+@\S+'), '', document) for document in module_list] #removing email addresses
  module_list = [re.sub(re.compile('www.\S+'), '', document) for document in module_list]
  module_list = [re.sub(re.compile('(((https|http)):((//)|(\\\\))+([\w\d:#@%/;$()~_?\+-=\\\.&](#!)?)*)'), '', document) for document in module_list]#removing html tags
  module_list = [re.sub(re.compile('(((pollev)).(com)+(\/\S+)*)'), '', document) for document in module_list] #removing pollev.com tags
  module_list = [re.sub(r'[^a-zA-Z\n\s]','', document) for document in module_list]
  
  return module_list

"""For QDA"""

qdaCorpus = []
qdaCorpus = preprocess (qda_list)

"""For ML"""

mlCorpus = []
mlCorpus = preprocess (ml_list)

"""For AI"""

aiCorpus = []
aiCorpus = preprocess (ai_list)

"""For DL"""

dlCorpus = []
dlCorpus = preprocess (dl_list)

"""For MD"""

mdCorpus = []
mdCorpus = preprocess (md_list)

"""For RPM

"""

rpmCorpus = []
rpmCorpus = preprocess (rpm_list)

len(aiCorpus)

# store the set of english stopwords from nltk as a variable to be used later

stop_words = set(stopwords.words('english'))


# create sets of certain words to remove
# this includes months of the year and certain words that appear 
# frequently in slides but don't have much to do with the meaning behind the document
# The extra gibberish-looking characters were retroactively added

calendar_months = {'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december'}
specific_stopwords =calendar_months | {'lecture', 'also','isabel','sassoon', 'isabelsassoon', 'yongmin','li','martin','shepperd','weibo','liu',
                                       'george','ghinea', 'jeff','wen','session','zhigang','monica', 'pereira', 'alessandro', 'pandini',
                                       'outline','contents','ian','blackman','xiaohui','dr','prof', 'atilla','abbasatilla',
                                       'hung-yi', 'lee', 'yi', 'hung', 'cmdscaledisthhv','hungyi', 'arxiv',
                                       'department of computer science','departmentofcomputerscience','lecturer','overview',
                                       'ieeeehhaa', 'mmssttggllii', 'eeccaaaa', 'oossyyccccuu', 'oollttuullrrhhuuuuffsseeaattttss', 'cceeeetteerreenniinnttrriiggiirrnne', 
                                       'iimmaagge', 'ssttrrfuuinccattluu', 'rerrseeult', 'histossriirrddeeicppaaeegga', 'aanniilggoottttreeiinniiffennaayyssse', 'uaaoo', 'lbbssnnfft', 'sddiiiippnnnn', 
                                       'iiggttxxddeerreerrooiilleennssuussggpptt', 'eevvhhbbnniiaasshhiittddnnooaaddeerrnniiiiccrrnncc', 'aaeeggssll', 'pppbbboooaaacccccccccsssooollleeekkktttmmmaaaggg', 
                                       'pppnnnrrrpppooorrr', 'eeeuuuuuuooonnnpppnnncccsss', 'dddiiiaaaeeemmm', 'tttnnnssseeeaaaooosss', 'gggfffiiisssiiioooeeennneeerrr', 'iorm', 'tequeequ', 'iiaammccaaqqgguueeiirr', 
                                       'eeiinn', 'hhffooiiggrrmmhh', 'aalleettiivvooeennl', 'xxxx','xxxxxxxxx',
                                       'iimmaaccaaqqgguueeiirr', 'iieenn', 'fflloooorrwwmm', 'llaaeettvviiooeennl', 'fffiiinnnaaall', 'aaannnaaalllyyysssiiisss', 'imaima', 'ansfansfequirequirsubssub',
                                       'iitt', 'xxiiyyii', 'llaatteexxiitt', 
                                       'sshhaabbaasseeiiffxxzzhheebbccaaeeccjjoollrrllwwhhrruueehhmmaaaaaabbxxiiccbbvvbbllssggnnbbeekkxxggnnvvjjuuggoommqqqqcckkmmllnnqqzzccooppccrrqqllmmaakkwwhhqqsszzooeennqqggjjzzggmmooyyffxxccoonncceebbcceewwrruuxxyyeewwccnnooyykkppcchhvvvvvvffvvllgguuppxxvvqqyyllzzxxvvttffxxccrrnnzzaattnnbbrrssookkyyttwwssmmqqjjqqyywwooppwwjjwwttnnmmccqqmmjjccvvhhwwggkkjjggffyyppccaaqqssssuuiiccaaffttmmhhoohhggnnwwzzqqrrrriimmxxbbzzrrllqqbbhhmmqqccggppddcceerroobbggiirruuddnnsskknnvvvvbbwweekkbbooxxffjjqqiioovviipplluuvveessuuwwiixxwwoossqqeevvjjttaaaarruuggcchhxxsswwwwttyyffooxxccggddiikkkkaahhrrrrppkkeeffqqqqttqqmmttggeeiiddvvnneeppxxppvvoovvllzzeerrjjookkmmlluuttttjjsseerrrrhhggddppmmuuqqjjaaggggmmkkppllbbeeeelljjiiyykkyyeevvnnccooyyuukkffllyyllppoossuuhhzzggiibbddwwddeevvwwrrllkkccaammvvqqaaggbbiittzzccmmxxyymmffvvkkvvvvppvvyyllaazz',
                                       'iiyyii', 'line', 'llaatteexxiitt', 
                                       'sshhaabbaasseeddcccceeccmmxxooiixxrrqqjjmmssyynnggeeddaabbeeggqqaaaaaabbxxiiccbbzzccssggnnbbffiibbppxxlluummttuubbqqaaddeeeehhccrrhhzzaabbmmwwssllbbiiwwffyywwzzxxyyyymmwwyyzznnvvmmzzssuullaallssbbsswwuussffuuxxffaammjjhhcchhiippnnpprrddwwmmffnnoocccckkeemmuuddppkkllsswwuullaakkvvwwttrrxxuubbwwbbttkkppffqqssssttmmiieeyyaaaaffeevvmmllppmmmmttnnwwiiiimmooaakkbbwweebbiinnddddvvqqvvqqssffttccssggxxzzggiissnnyygguummddooeezzyyttmmiiiizzmmqqjjbbxxccggxxyyppddqqrruuvvvvzzhhppxxuuttnnkkjjcceevvaarrttoooorrhhllppoorrttkkeeffttqqppooggsskkaajjbboommaarrvvddwwmmvveeiihhrrqqnniijjyymmiiccmmrrggooyyuuiiqqllaallmmzzyyddxxnnyyccrrppzzoozzmmjjkkmmmmggkkwwccllooooddiizzggmmuukkrroonnhhnnaarrddiizzkkyyjjllddhhrrkkggffccwwrrffuuwwddggppuuddllppyywwkkwwmmllyyddooiiaassuuhhaaggffbbiieekkttssbbggiiaahheeiijjnnssmmppoovvooiinnwwbbooeexxffggllbbppiiggttllggllaatteexxiitt', 'sshhaabbaasseeiiffxxzzhheebbccaaeeccjjoollrrllwwhhrruueehhmmaaaaaabbxxiiccbbvvbbllssggnnbbeekkxxggnnvvjjuuggoommqqqqcckkmmllnnqqzzccooppccrrqqllmmaakkwwhhqqsszzooeennqqggjjzzggmmooyyffxxccoonncceebbcceewwrruuxxyyeewwccnnooyykkppcchhvvvvvvffvvllgguuppxxvvqqyyllzzxxvvttffxxccrrnnzzaattnnbbrrssookkyyttwwssmmqqjjqqyywwooppwwjjwwttnnmmccqqmmjjccvvhhwwggkkjjggffyyppccaaqqssssuuiiccaaffttmmhhoohhggnnwwzzqqrrrriimmxxbbzzrrllqqbbhhmmqqccggppddcceerroobbggiirruuddnnsskknnvvvvbbwweekkbbooxxffjjqqiioovviipplluuvveessuuwwiixxwwoossqqeevvjjttaaaarruuggcchhxxsswwwwttyyffooxxccggddiikkkkaahhrrrrppkkeeffqqqqttqqmmttggeeiiddvvnneeppxxppvvoovvllzzeerrjjookkmmlluuttttjjsseerrrrhhggddppmmuuqqjjaaggggmmkkppllbbeeeelljjiiyykkyyeevvnnccooyyuukkffllyyllppoossuuhhzzggiibbddwwddeevvwwrrllkkccaammvvqqaaggbbiittzzccmmxxyymmffvvkkvvvvppvvyyllaazzzzzzaahhggvvffaacckkwwjjaallaatteexxiitt',
                                       'sshhaabbaasseeddcccceeccmmxxooiixxrrqqjjmmssyynnggeeddaabbeeggqqaaaaaabbxxiiccbbzzccssggnnbbffiibbppxxlluummttuubbqqaaddeeeehhccrrhhzzaabbmmwwssllbbiiwwffyywwzzxxyyyymmwwyyzznnvvmmzzssuullaallssbbsswwuussffuuxxffaammjjhhcchhiippnnpprrddwwmmffnnoocccckkeemmuuddppkkllsswwuullaakkvvwwttrrxxuubbwwbbttkkppffqqssssttmmiieeyyaaaaffeevvmmllppmmmmttnnwwiiiimmooaakkbbwweebbiinnddddvvqqvvqqssffttccssggxxzzggiissnnyygguummddooeezzyyttmmiiiizzmmqqjjbbxxccggxxyyppddqqrruuvvvvzzhhppxxuuttnnkkjjcceevvaarrttoooorrhhllppoorrttkkeeffttqqppooggsskkaajjbboommaarrvvddwwmmvveeiihhrrqqnniijjyymmiiccmmrrggooyyuuiiqqllaallmmzzyyddxxnnyyccrrppzzoozzmmjjkkmmmmggkkwwccllooooddiizzggmmuukkrroonnhhnnaarrddiizzkkyyjjllddhhrrkkggffccwwrrffuuwwddggppuuddllppyywwkkwwmmllyyddooiiaassuuhhaaggffbbiieekkttssbbggiiaahheeiijjnnssmmppoovvooiinnwwbboo',
                                       'mmmaaaggg', 'pppnnpppooorrr', 'eeeuuuuuuooonn','itthhaass', 'pppbbboooaaacccccccccsssooollleeekk','itthhaass', 'pppbbboooaaacccccccccsssooollleeekk','ussggpptt', 'eevvhhbbnniiaasshhiittddnnooaaddeerrn','aeeggssll', 'eerrssuueeaassppssiirraannuuccttg',
                                       'ussggpptt', 'eevvhhbbnniiaasshhiittddnnooaaddeerrn','aeeggssll', 'eerrssuueeaassppssiirraannuuccttgglliitthhaass','iiippnnnn', 'iiggttxxddeerreerrooiilleen',
                                       'learning outcomes', 'outcomes','agmorrreaacabhsdlsaeolrawdnd', 'refwildchrisjandmaxinepfannkuchstatisticalthinkinginempiricalenquiryinternationalstatistical', 
                                       'summary', 'references', 'cid','week','brunelacuk','th','st', 
                                       'brunel university london', 'bruneluniversitylondon', 'brunel', 'university', 'london'}

# Stemming

def stemFunc(stemCorpus):
  stemmer = SnowballStemmer('english')  
  stemmedWords = [
    [stemmer.stem(word) for word in corpora.lower().split() if word not in stop_words and word not in specific_stopwords and len(word) > 3]
    for corpora in stemCorpus
    ]
  return stemmedWords
  # print(stemmedWords)

# Lemmatization

def lemmFunc(lemmCorpus):
  lemmatizer = WordNetLemmatizer()

  lemmWords = [
    [lemmatizer.lemmatize(word) for word in corpora.lower().split() if word not in stop_words and word not in specific_stopwords and len(word) > 3]
    for corpora in lemmCorpus
    ]
  return lemmWords  
  # print(lemmWords)

qdaStem = stemFunc(qdaCorpus)
mlStem = stemFunc(mlCorpus)
aiStem = stemFunc(aiCorpus)
mdStem = stemFunc(mdCorpus)
dlStem = stemFunc(dlCorpus)
rpmStem = stemFunc(rpmCorpus)

qdaLemm = lemmFunc(qdaCorpus)
mlLemm = lemmFunc(mlCorpus)
aiLemm = lemmFunc(aiCorpus)
mdLemm = lemmFunc(mdCorpus)
dlLemm = lemmFunc(dlCorpus)
rpmLemm = lemmFunc(rpmCorpus)

"""# Representation methods

Bag of words
"""

'''
Create a dictionary from 'processed_docs' containing the number of times a word appears 
in the training set using gensim.corpora.Dictionary and call it 'dictionary'
'''
# dictionary
qda_dict = gensim.corpora.Dictionary(qdaLemm)
md_dict = gensim.corpora.Dictionary(mdLemm)
ml_dict = gensim.corpora.Dictionary(mlLemm)
rpm_dict = gensim.corpora.Dictionary(rpmLemm)
ai_dict = gensim.corpora.Dictionary(aiLemm)
dl_dict = gensim.corpora.Dictionary(dlLemm)

'''
Function for checking dictionary created
'''
def check_dict(dictionary):  
  count = 0
  for k, v in dictionary.iteritems():
      print(k, v)
      count += 1
      if count > 30:
          break

'''
Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many
words and how many times those words appear. Save this to 'bow_corpus'
'''
def bag_of_words(dictionary, lemmatizedWords):
  bow_corpus = [dictionary.doc2bow(doc) for doc in lemmatizedWords]

  return bow_corpus

# bow_corpus
qda_bow = bag_of_words(qda_dict, qdaLemm)
md_bow = bag_of_words(md_dict , mdLemm)
ml_bow = bag_of_words(ml_dict, mlLemm)
rpm_bow = bag_of_words(rpm_dict , rpmLemm)
ai_bow = bag_of_words(ai_dict , aiLemm )
dl_bow = bag_of_words(dl_dict, dlLemm )

"""TF-IDF

"""

# Word weight in Bag of Words corpus
def word_weight(bow_corpus, dictionary):
  weight_of_word =[]
  for doc in bow_corpus:
    for id, freq in doc:
      weight_of_word.append([dictionary[id], freq])
  return weight_of_word

qda_weight = word_weight(qda_bow, qda_dict)
ml_weight = word_weight(ml_bow, ml_dict)
rpm_weight = word_weight(rpm_bow, rpm_dict)
dl_weight = word_weight(dl_bow, dl_dict)
ai_weight = word_weight(ai_bow, ai_dict)
md_weight = word_weight(md_bow, md_dict)

# type(rpm_weight)

# function to calculate tf-idf word weight
def tfidf_word_weight(tfidf,bow_corpus,dictionary):
  weight_tfidf =[]
  for doc in tfidf[bow_corpus]:
    for id, freq in doc:
      weight_tfidf.append([dictionary[id], np.around(freq, decimals = 3)])
  print(weight_tfidf)

# create tf-idf model
qda_tf_idf = gensim.models.TfidfModel(qda_bow, smartirs ='ntc')

qda_tf_weight = tfidf_word_weight(qda_tf_idf,qda_bow,qda_dict)

ml_tf_idf = gensim.models.TfidfModel(ml_bow, smartirs ='ntc')

ml_tf_weight = tfidf_word_weight(ml_tf_idf, ml_bow, ml_dict)

ai_tf_idf = gensim.models.TfidfModel(ai_bow, smartirs ='ntc')

ai_tf_weight = tfidf_word_weight(ai_tf_idf, ai_bow, ai_dict)

md_tf_idf = gensim.models.TfidfModel(md_bow, smartirs ='ntc')

md_tf_weight = tfidf_word_weight(md_tf_idf, md_bow, md_dict)

dl_tf_idf = gensim.models.TfidfModel(dl_bow, smartirs ='ntc')

dl_tf_weight = tfidf_word_weight(dl_tf_idf, dl_bow, dl_dict)

rpm_tf_idf = gensim.models.TfidfModel(rpm_bow, smartirs ='ntc')

rpm_tf_weight = tfidf_word_weight(rpm_tf_idf, rpm_bow, rpm_dict)



"""Bigrams and Trigrams

Bigrams
"""

qdaBigram = Phrases(qdaLemm, min_count = 3, threshold = 10)
mdBigram = Phrases(mdLemm, min_count = 3, threshold = 10)
aiBigram = Phrases(aiLemm, min_count = 3, threshold = 10)
dlBigram = Phrases(dlLemm, min_count = 3, threshold = 10)
rpmBigram = Phrases(rpmLemm, min_count = 3, threshold = 10)
mlBigram = Phrases(mlLemm, min_count = 3, threshold = 10)

print(mdBigram[mdLemm[0]])

"""Trigrams"""

qdaTrigram = Phrases(qdaBigram[qdaLemm], threshold = 10)
dlTrigram = Phrases(dlBigram[dlLemm], threshold = 10)
aiTrigram = Phrases(aiBigram[aiLemm], threshold = 10)
mdTrigram = Phrases(mdBigram[mdLemm], threshold = 10)
rpmTrigram = Phrases(rpmBigram[rpmLemm], threshold = 10)
mlTrigram = Phrases(mlBigram[mlLemm], threshold = 10)

print(mdTrigram[mdBigram[mdLemm[0]]])

"""Word2Vec"""

from multiprocessing import cpu_count
from gensim.models.word2vec import Word2Vec

def convert_w2v(lemmWords):
  w2v_model = Word2Vec(lemmWords, min_count = 0, workers = cpu_count())

  return w2v_model

import gensim.downloader as api
from gensim.matutils import softcossim
from gensim import corpora
import jieba

from gensim.models import Word2Vec

"""QDA"""

qda_w2v = convert_w2v(qdaLemm)

"""AI"""

ai_w2v = convert_w2v(aiLemm)

"""DL"""

dl_w2v = convert_w2v(dlLemm)

"""RPM"""

rpm_w2v = convert_w2v(rpmLemm)

"""ML"""

ml_w2v = convert_w2v(mlLemm)

"""MD"""

md_w2v = convert_w2v(mdLemm)

# print(w2v_model['statistical'])

# w1 = 'statistical'
# w2v_model.wv.most_similar(positive=w1, topn=12)

"""# EDA

Word Cloud
"""

#@title
import io

#@title
# AI word cloud

tplot = ''
for cat in aiLemm:
  cat = str(cat)
  tokens = cat.split()
  for i in range(len(tokens)):
    tokens[i] = tokens[i].lower()
    tplot += " ".join(tokens)

word_cloud = WordCloud(collocations = False, 
                       width=1920, height = 1080,
                       background_color = 'white').generate(tplot)



plt.figure(figsize=(16, 16), facecolor = None)
plt.imshow(WordCloud().generate(tplot))
plt.axis('off')
# plt.show()

# plt.savefig("/Images/AIWC.png", format="png", dpi=1200)

word_cloud.to_file("/Images/AIWC.png")

#@title
# MD word cloud

tplot = ''
for cat in mdLemm:
  cat = str(cat)
  tokens = cat.split()
  for i in range(len(tokens)):
    tokens[i] = tokens[i].lower()
    tplot += " ".join(tokens)

word_cloud = WordCloud(collocations = False, 
                       width=1920, height = 1080,
                       background_color = 'white').generate(tplot)



plt.figure(figsize=(16, 16), facecolor = None)
plt.imshow(WordCloud().generate(tplot))
plt.axis('off')
# plt.show()

# plt.savefig("/Images/MDWC.png", format="png", dpi=1200)

word_cloud.to_file("/Images/MDWC.png")

#@title
# QDA word cloud

tplot = ''
for cat in qdaLemm:
  cat = str(cat)
  tokens = cat.split()
  for i in range(len(tokens)):
    tokens[i] = tokens[i].lower()
    tplot += " ".join(tokens)

word_cloud = WordCloud(collocations = False, 
                       width=1920, height = 1080,
                       background_color = 'white').generate(tplot)



plt.figure(figsize=(16, 16), facecolor = None)
plt.imshow(WordCloud().generate(tplot))
plt.axis('off')
# plt.show()

# plt.savefig("/Images/QDAWC.png", format="png", dpi=1200)

word_cloud.to_file("/Images/QDAWC.png")

#@title
# ML word cloud

tplot = ''
for cat in mlLemm:
  cat = str(cat)
  tokens = cat.split()
  for i in range(len(tokens)):
    tokens[i] = tokens[i].lower()
    tplot += " ".join(tokens)

word_cloud = WordCloud(collocations = False, 
                       width=1920, height = 1080,
                       background_color = 'white').generate(tplot)



plt.figure(figsize=(16, 16), facecolor = None)
plt.imshow(WordCloud().generate(tplot))
plt.axis('off')
# plt.show()

# plt.savefig("/Images/MLWC.png", format="png", dpi=1200)

word_cloud.to_file("/Images/MLWC.png")

#@title
# DL word cloud

tplot = ''
for cat in dlLemm:
  cat = str(cat)
  tokens = cat.split()
  for i in range(len(tokens)):
    tokens[i] = tokens[i].lower()
    tplot += " ".join(tokens)

word_cloud = WordCloud(collocations = False, 
                       width=1920, height = 1080,
                       background_color = 'white').generate(tplot)

# cloud = WordCloud(width=200/my_dpi,height=150/my_dpi)
# plt.imshow(cloud.generate_from_frequencies(t))
# plt.savefig('c.png', dpi=my_dpi)
# plt.clf()

plt.figure(figsize=(16, 16), facecolor = None)
plt.imshow(WordCloud().generate(tplot))
plt.axis('off')
# plt.show()

# plt.savefig("/Images/DLWC.png", format="png", dpi=1200)

word_cloud.to_file("/Images/DLWC.png")

#@title
# RPM word cloud

tplot = ''
for cat in rpmLemm:
  cat = str(cat)
  tokens = cat.split()
  for i in range(len(tokens)):
    tokens[i] = tokens[i].lower()
    tplot += " ".join(tokens)

word_cloud = WordCloud(collocations = False, 
                       width=1920, height = 1080,
                       background_color = 'white').generate(tplot)



plt.figure(figsize=(16, 16), facecolor = None)
plt.imshow(WordCloud().generate(tplot))
plt.axis('off')
# plt.show()


# plt.savefig("/Images/RPMWC.png", format="png", dpi=1200)
word_cloud.to_file("/Images/RPMWC.png")

#@title

"""Word Freq Histograms"""

#@title
def WordFreq(lemmWords, numOfWordsToDisplay):
  # List of all words across tweets
  all_words = list(itertools.chain(*lemmWords))

  # Create counter
  counts_words = collect.Counter(all_words)

  counts_words.most_common(numOfWordsToDisplay)


  counts_words_df = pd.DataFrame(counts_words.most_common(numOfWordsToDisplay),
                             columns=['words', 'count'])

  return counts_words_df

#@title
AI_Freq = WordFreq(aiLemm, 30)
ML_Freq = WordFreq(mlLemm, 30)
QDA_Freq = WordFreq(qdaLemm, 30)
RPM_Freq = WordFreq(rpmLemm, 30)
MD_Freq = WordFreq(mdLemm, 30)
DL_Freq = WordFreq(dlLemm, 30)

"""AI

"""

#@title
fig, ax = plt.subplots(figsize=(8, 8))

# Plot horizontal bar graph
AI_Freq.sort_values(by='count').plot.barh(x='words',
                      y='count',
                      ax=ax)

ax.set_title("Most Frequently Used Words in the CS5707 Module (Artificial Intelligence)")

# plt.show()
plt.savefig("/Images/AIWordFreq.png", format="png", dpi=1200)

"""QDA"""

#@title
fig, ax = plt.subplots(figsize=(8, 8))

# Plot horizontal bar graph
QDA_Freq.sort_values(by='count').plot.barh(x='words',
                      y='count',
                      ax=ax)

ax.set_title("Most Frequently Used Words in the CS5701 Module (Quantitative Data Analysis)")

# plt.show()
plt.savefig("/Images/QDAWordFreq.png", format="png", dpi=1200)

"""RPM"""

#@title
fig, ax = plt.subplots(figsize=(8, 8))

# Plot horizontal bar graph
RPM_Freq.sort_values(by='count').plot.barh(x='words',
                      y='count',
                      ax=ax)

ax.set_title("Most Frequently Used Words in the CS5704 Module (Research Project Management)")

# plt.show()
plt.savefig("/Images/RPMWordFreq.png", format="png", dpi=1200)

"""DL"""

#@title
fig, ax = plt.subplots(figsize=(8, 8))

# Plot horizontal bar graph
DL_Freq.sort_values(by='count').plot.barh(x='words',
                      y='count',
                      ax=ax)

ax.set_title("Most Frequently Used Words in the CS5708 Module (Deep Learning)")

# plt.show()
plt.savefig("/Images/DLWordFreq.png", format="png", dpi=1200)

"""ML"""

#@title
fig, ax = plt.subplots(figsize=(8, 8))

# Plot horizontal bar graph
ML_Freq.sort_values(by='count').plot.barh(x='words',
                      y='count',
                      ax=ax)

ax.set_title("Most Frequently Used Words in the CS5706 Module (Machine Learning)")

# plt.show()
plt.savefig("/Images/MLWordFreq.png", format="png", dpi=1200)

"""MD"""

#@title
fig, ax = plt.subplots(figsize=(8, 8))

# Plot horizontal bar graph
MD_Freq.sort_values(by='count').plot.barh(x='words',
                      y='count',
                      ax=ax)

ax.set_title("Most Frequently Used Words in the CS5702 Module (Modern Data)")

# plt.show()
plt.savefig("/Images/MDWordFreq.png", format="png", dpi=1200)

"""Combined Modules

"""

#@title
# All cleaned data for all modules combined into one list

dataLemm = aiLemm + rpmLemm + mdLemm + qdaLemm + mlLemm + dlLemm

# # @title
# # Combined Dataset word cloud
# # Uses a lot of working memory. Needs to be ran on > 13GB of memory. 

# tplot = ''
# for cat in dataLemm:
#   cat = str(cat)
#   tokens = cat.split()
#   for i in range(len(tokens)):
#     tokens[i] = tokens[i].lower()
#     tplot += " ".join(tokens)

# word_cloud = WordCloud(collocations = False, 
#                        width=1920, height = 1080,
#                        background_color = 'white').generate(tplot)



# plt.figure(figsize=(16, 16), facecolor = None)
# plt.imshow(WordCloud().generate(tplot))
# plt.axis('off')
# # plt.show()



# word_cloud.to_file("/Images/CombinedWC.png")

#@title
Full_dataset_freq = WordFreq(dataLemm, 40)

fig, ax = plt.subplots(figsize=(8, 8))

# Plot horizontal bar graph
Full_dataset_freq.sort_values(by='count').plot.barh(x='words',
                      y='count',
                      ax=ax)

ax.set_title("Most Frequently Used Words in the Full Dataset of Modules")

# plt.show()
plt.savefig("/Images/FullWordFreq.png", format="png", dpi=1200)

#@title
def network_words(lemmWords, numOfWordsToDisplay):
  # List of all words across tweets

  terms_bigram = [list(bigrams(tok)) for tok in lemmWords]
  word_bigrams = list(itertools.chain(*terms_bigram))

  # Create counter of words in clean bigrams
  bigram_counts = collect.Counter(word_bigrams)

  bigram_counts.most_common(numOfWordsToDisplay)

  bigram_df = pd.DataFrame(bigram_counts.most_common(numOfWordsToDisplay),
                             columns=['bigram', 'count'])


  return bigram_df

#@title
def network_trigram(lemmWords, numOfWordsToDisplay):
  # List of all words across tweets

  terms_trigram = [list(trigrams(tok)) for tok in lemmWords]
  word_trigrams = list(itertools.chain(*terms_trigram))

  trigram_counts = collect.Counter(word_trigrams)

  trigram_counts.most_common(numOfWordsToDisplay)

  trigram_df = pd.DataFrame(trigram_counts.most_common(numOfWordsToDisplay),
                              columns=['bigram', 'count'])



  return trigram_df

"""AI"""

#@title
AINet = network_words(aiLemm, 20)
QDANet = network_words(qdaLemm, 20)
RPMNet = network_words(rpmLemm, 20)
MDNet = network_words(mdLemm, 20)
MLNet = network_words(mlLemm, 20)
DLNet = network_words(dlLemm, 20)

#@title
AITriNet = network_trigram(aiLemm, 20)
QDATriNet = network_trigram(qdaLemm, 20)
RPMTriNet = network_trigram(rpmLemm, 20)
MDTriNet = network_trigram(mdLemm, 20)
MLTriNet = network_trigram(mlLemm, 20)
DLTriNet = network_trigram(dlLemm, 20)

#@title
d = AINet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/AIWordNet.png", format="png", dpi=1200)

#@title
d = AITriNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/AITriNet.png", format="png", dpi=1200)

"""QDA"""

#@title
d = QDANet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/QDAWordNet.png", format="png", dpi=1200)

#@title
d = QDATriNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/QDATriNet.png", format="png", dpi=1200)

"""RPM"""

#@title
d = RPMNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/RPMWordNet.png", format="png", dpi=1200)

#@title
d = RPMTriNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/RPMTriNet.png", format="png", dpi=1200)

"""MD"""

#@title
d = MDNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/MDWordNet.png", format="png", dpi=1200)

#@title
d = MDTriNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/MDTriNet.png", format="png", dpi=1200)

"""ML"""

#@title
d = MLNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/MLWordNet.png", format="png", dpi=1200)

#@title
d = MLTriNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/MLTriNet.png", format="png", dpi=1200)

"""DL"""

#@title
d = DLNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/DLWordNet.png", format="png", dpi=1200)

#@title
d = DLTriNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(12, 12))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/DLTriNet.png", format="png", dpi=1200)

"""FULL Word Net"""

#@title
FullDataNet = network_words(dataLemm, 30)
FullDataTriNet = network_trigram(dataLemm, 30)

#@title
d = FullDataNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(16, 16))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/FullWordNet.png", format="png", dpi=1200)

#@title
d = FullDataTriNet.set_index('bigram').T.to_dict('records')
G = nx.Graph()

# Create connections between nodes
for k, v in d[0].items():
    G.add_edge(k[0], k[1], weight=(v * 10))

fig, ax = plt.subplots(figsize=(16, 16))

pos = nx.spring_layout(G, k=2)

# Plot networks
nx.draw_networkx(G, pos,
                 font_size=16,
                 width=3,
                 edge_color='grey',
                 node_color='pink',
                 with_labels = False,
                 ax=ax)

# Create offset labels
for key, value in pos.items():
    x, y = value[0]+.135, value[1]+.045
    ax.text(x, y,
            s=key,
            bbox=dict(facecolor='red', alpha=0.25),
            horizontalalignment='center', fontsize=13)
    
# plt.show()
plt.savefig("/Images/FullTriNet.png", format="png", dpi=1200)

# stop



"""----
----

# Some Extraction Algorithms

LDA

Using LDA Multicore
"""

# Function to plot LDA graph
def lda_plot(model, bow, dictionary):

  lda_display = pyLDAvis.gensim_models.prepare(model, bow, dictionary, mds='mmds')
  return pyLDAvis.display(lda_display)

# Latent dirichlet allocation fucntion

def lda_multi(bow, module_file_order, dictionary):

  lda_model =  gensim.models.LdaMulticore(bow, 
                                   num_topics = len(module_file_order), 
                                   id2word = dictionary, 
                                   iterations = 100,                                   
                                   passes = 100,
                                   workers = 2)
  print(module_file_order)
  print("\n")

  for idx, topic in lda_model.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic ))
    print("\n")

  return lda_model

"""AI"""

ai_lda = lda_multi(ai_bow,ai_file_order,ai_dict)

lda_plot(ai_lda, ai_bow, ai_dict)

"""DL"""

dl_lda = lda_multi(dl_bow,dl_file_order,dl_dict)

lda_plot(dl_lda, dl_bow, dl_dict)

"""QDA"""

qda_lda = lda_multi(qda_bow,qda_file_order,qda_dict)

lda_plot(qda_lda, qda_bow, qda_dict)

"""MD"""

md_lda = lda_multi(md_bow,md_file_order,md_dict)

lda_plot(md_lda, md_bow, md_dict)

"""ML"""

ml_lda = lda_multi(ml_bow,ml_file_order,ml_dict)

lda_plot(ml_lda, ml_bow, ml_dict)

"""RPM"""

rpm_lda = lda_multi(rpm_bow,rpm_file_order,rpm_dict)

lda_plot(rpm_lda, rpm_bow, rpm_dict)

# Set up coherence model
# c_uci 
# u_mass
# c_v
score = []
coherence_model_lda = CoherenceModel(model=qda_lda, texts = qdaLemm, corpus = qda_bow,  coherence='u_mass')


# CoherenceModel(model=lda_model, corpus=qda_bow, dictionary=qda_dict, coherence='u_mass')
# Calculate and print coherence

# for i in range (len(rpm_file_order)):

with np.errstate(invalid='ignore'):
    
  coherence_lda = coherence_model_lda.get_coherence()

# score.append(coherence_lda.get_coherence())

print('\nCoherence Score:', coherence_lda)

"""TextRank

"""

print(ai_file_order,"\n\n",
qda_file_order,"\n\n",
md_file_order,"\n\n",
ml_file_order,"\n\n",
dl_file_order,"\n\n",
rpm_file_order)

def TextRank(lemmatizedWords):
  newLemm = lemmatizedWords
  for i in range(len(lemmatizedWords)):
    newLemm[i] = ' '.join(lemmatizedWords[i])


    return(keywords.keywords(newLemm[i]))

def unwind(lemmatizedWords):
  newLemm = lemmatizedWords
  for i in range(len(lemmatizedWords)):
    newLemm[i] = ' '.join(lemmatizedWords[i])

  return newLemm

"""RPM"""

rpm_tr = TextRank(rpmLemm)

print(rpm_tr)

"""AI"""

ai_tr = TextRank(aiLemm)

print(ai_tr)

"""ML"""

ml_tr = TextRank(mlLemm)

print(ml_tr)

"""MD"""

md_tr = TextRank(mdLemm)

print(md_tr)

"""QDA"""

qda_tr = TextRank(qdaLemm)

print(qda_tr)

"""DL"""

dl_tr = TextRank(dlLemm)

print(dl_tr)

"""RAKE"""

!pip install python-rake

import RAKE
import operator

# RAKE stopwords directory
stop_dir = "/Data/SmartStoplist.txt"

rake_object = RAKE.Rake(stop_dir)

def Sort_Tuple(tup):

  # reverse = None (Sorts in Ascending order)
  # key is set to sort using second element of 
  # sublist lambda has been used 
  tup.sort(key = lambda x: x[1])
  return tup

def rake_code(unlemm):
  for i in range(len(unlemm)):
    keywords = Sort_Tuple(rake_object.run(unlemm[i]))[-10:]
    print("Doc {} keywords: ".format(i), keywords)

"""AI"""

aiUnlemm = unwind(aiLemm)

rake_code(aiUnlemm)

"""DL"""

dlUnlemm = unwind(dlLemm)

rake_code(dlUnlemm)

"""QDA"""

qdaUnlemm = unwind(qdaLemm)

rake_code(qdaUnlemm)

"""ML"""

mlUnlemm = unwind(mlLemm)

rake_code(mlUnlemm)

"""MD"""

mdUnlemm = unwind(mdLemm)

rake_code(mdUnlemm)

"""RPM"""

rpmUnlemm = unwind(rpmLemm)

rake_code(rpmUnlemm)

"""YAKE"""

!pip install git+https://github.com/LIAAD/yake

import yake

kw_extractor = yake.KeywordExtractor()

def yake(unlemm):
  for i in range (len(unlemm)):
    yake_keywords = kw_extractor.extract_keywords(unlemm[i])
    
    for kw in yake_keywords:
      print("Doc {} keywords: ".format(i), kw)

# for i in range (len(aiUnlemm)):
#     yake_keywords = kw_extractor.extract_keywords(aiUnlemm[i])
    
#     for kw in yake_keywords:
#       print(kw)

# from yake.highlight import TextHighlighter

# th = TextHighlighter(max_ngram_size = 3)
# th.highlight(lemmWords[5], keywords)

qdaLemm = lemmFunc(qdaCorpus)
mlLemm = lemmFunc(mlCorpus)
aiLemm = lemmFunc(aiCorpus)
mdLemm = lemmFunc(mdCorpus)
dlLemm = lemmFunc(dlCorpus)
rpmLemm = lemmFunc(rpmCorpus)

"""AI"""

aiUnlemm = unwind(aiLemm)

print(yake(aiUnlemm))

"""DL"""

# lowest prob on top highest below but vals so small and so close

dlUnlemm = unwind(dlLemm)
print(yake(dlUnlemm))

"""QDA"""

qdaUnlemm = unwind(qdaLemm)
print(yake(qdaUnlemm))

"""ML"""

mlUnlemm = unwind(mlLemm)
print(yake(mlUnlemm))

"""MD"""

mdUnlemm = unwind(mdLemm)
print(yake(mdUnlemm))

"""RPM"""

rpmUnlemm = unwind(rpmLemm)
print(yake(rpmUnlemm))

"""KeyBERT"""

!pip install keybert
# !pip install keybert[flair]
# !pip install keybert[gensim]
# !pip install keybert[spacy]
# !pip install keybert[use]

# !pip install Pillow==9.0.0

from keybert import KeyBERT

qdaLemm = lemmFunc(qdaCorpus)
mlLemm = lemmFunc(mlCorpus)
aiLemm = lemmFunc(aiCorpus)
mdLemm = lemmFunc(mdCorpus)
dlLemm = lemmFunc(dlCorpus)
rpmLemm = lemmFunc(rpmCorpus)

def run_keybert(unlemm):
  kw_model = KeyBERT()

  keywords = kw_model.extract_keywords(unlemm)
  return(keywords)

def run_keybert_ngram(unlemm, num):
  kw_model = KeyBERT()
  keywords2 = kw_model.extract_keywords(unlemm, keyphrase_ngram_range= (1,num), stop_words= None)

  return (keywords2)

"""AI"""

ai_kb = run_keybert(aiUnlemm)

for i in range(len(ai_kb)):
  print ("Doc {}: ".format(i),ai_kb[i])

ai_kb_n = run_keybert_ngram(aiUnlemm,2)

for i in range(len(ai_kb_n)):
  print ("Doc {}: ".format(i),ai_kb_n[i])

ai_kb_n = run_keybert_ngram(aiUnlemm,3)

for i in range(len(ai_kb_n)):
  print ("Doc {}: ".format(i),ai_kb_n[i])

"""DL"""

dl_kb = run_keybert(dlUnlemm)

for i in range(len(dl_kb)):
  print ("Doc {}: ".format(i),dl_kb[i])

dl_kb_n = run_keybert_ngram(dlUnlemm,2)

for i in range(len(dl_kb_n)):
  print ("Doc {}: ".format(i),dl_kb_n[i])

dl_kb_n = run_keybert_ngram(dlUnlemm,3)

for i in range(len(dl_kb_n)):
  print ("Doc {}: ".format(i),dl_kb_n[i])

"""QDA"""

qda_kb = run_keybert(qdaUnlemm)

for i in range(len(qda_kb)):
  print ("Doc {}: ".format(i),qda_kb[i])

qda_kb_n = run_keybert_ngram(qdaUnlemm,2)

for i in range(len(qda_kb_n)):
  print ("Doc {}: ".format(i),qda_kb_n[i])

qda_kb_n = run_keybert_ngram(qdaUnlemm,3)

for i in range(len(qda_kb_n)):
  print ("Doc {}: ".format(i),qda_kb_n[i])

"""ML"""

ml_kb = run_keybert(mlUnlemm)

for i in range(len(ml_kb)):
  print ("Doc {}: ".format(i),ml_kb[i])

ml_kb_n = run_keybert_ngram(mlUnlemm,2)

for i in range(len(ml_kb_n)):
  print ("Doc {}: ".format(i),ml_kb_n[i])

ml_kb_n = run_keybert_ngram(mlUnlemm,3)

for i in range(len(ml_kb_n)):
  print ("Doc {}: ".format(i),ml_kb_n[i])

"""MD"""

md_kb = run_keybert(mdUnlemm)

for i in range(len(md_kb)):
  print ("Doc {}: ".format(i),md_kb[i])

md_kb_n = run_keybert_ngram(mdUnlemm,2)

for i in range(len(md_kb_n)):
  print ("Doc {}: ".format(i),md_kb_n[i])

md_kb_n = run_keybert_ngram(mdUnlemm,3)

for i in range(len(md_kb_n)):
  print ("Doc {}: ".format(i),md_kb_n[i])

"""RPM"""

rpm_kb = run_keybert(rpmUnlemm)

for i in range(len(rpm_kb)):
  print ("Doc {}: ".format(i),rpm_kb[i])

rpm_kb_n = run_keybert_ngram(rpmUnlemm,2)

for i in range(len(rpm_kb_n)):
  print ("Doc {}: ".format(i),rpm_kb_n[i])

rpm_kb_n = run_keybert_ngram(rpmUnlemm,3)

for i in range(len(rpm_kb_n)):
  print ("Doc {}: ".format(i),rpm_kb_n[i])